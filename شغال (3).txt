الحمد لله
اشتغل اوبن هاندس














%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
docker pull docker.all-hands.dev/all-hands-ai/runtime:0.12-nikolaik

KeyboardInterrupt:
m@DESKTOP-MUB16F8:~$ docker run -it --privileged --rm \
LLM_API>   -e LLM_API_KEY="ollama" \
>   -e OLLAMA_HOST="http://172.21.151.42:11434" \
>   -e LLM_OLLAMA_BASE_URL="http://172.21.151.42:11434" \
>   -e OLLAMA_BASE_URL="http://172.21.151.42:11434" \
>   -e OLLAMA_URL="http://172.21.151.42:11434" \
>   -e SANDBOX_RUNTIME_CONTAINER_IMAGE="docker.all-hands.dev/all-hands-ai/runtime:0.12-nikolaik" \
>   -v /var/run/docker.sock:/var/run/docker.sock \
>   -p 3000:3000 \
>   --name openhands-app \
>   docker.all-hands.dev/all-hands-ai/openhands:0.12
Starting OpenHands...
Running OpenHands as root
INFO:     Started server process [10]



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


docker run -it --privileged --rm \
  -e LLM_API_KEY="ollama" \
  -e OLLAMA_HOST="http://172.21.151.42:11434" \
  -e LLM_OLLAMA_BASE_URL="http://172.21.151.42:11434" \
  -e OLLAMA_BASE_URL="http://172.21.151.42:11434" \
  -e OLLAMA_URL="http://172.21.151.42:11434" \
  -e SANDBOX_RUNTIME_CONTAINER_IMAGE="docker.all-hands.dev/all-hands-ai/runtime:0.12-nikolaik" \
  -v /var/run/docker.sock:/var/run/docker.sock \
  -p 3000:3000 \
  --name openhands-app \
  docker.all-hands.dev/all-hands-ai/openhands:0.12















###################################################hالاوامر كاملة التى تم تجريبها

# 1. تشغيل Ollama (نفذها في الخلفية إن لم تكن شغالة بالفعل)
ollama serve

# 2. معرفة عنوان الـ IP الخاص بك داخل WSL2
hostname -I

# 3. اختبار الاتصال بـ Ollama من الجهاز المحلي أو من داخل أي حاوية
curl http://172.21.151.42:11434/api/tags

# 4. (اختياري) تجربة نموذج Ollama مباشرة من داخل الحاوية
curl http://172.21.151.42:11434/api/generate -X POST -H "Content-Type: application/json" -d '{
  "model": "llama3:latest",
  "prompt": "What is the capital of France?",
  "stream": false
}'

# 5. تشغيل OpenHands مع ضبط جميع متغيرات البيئة على عنوان Ollama الصحيح (نفّذ هذا في الجهاز الأساسي، ليس داخل حاوية)
docker run -it --privileged --rm \
  -e LLM_API_KEY="ollama" \
  -e OLLAMA_HOST="http://172.21.151.42:11434" \
  -e LLM_OLLAMA_BASE_URL="http://172.21.151.42:11434" \
  -e OLLAMA_BASE_URL="http://172.21.151.42:11434" \
  -e OLLAMA_URL="http://172.21.151.42:11434" \
  -e SANDBOX_RUNTIME_CONTAINER_IMAGE="docker.all-hands.dev/all-hands-ai/runtime:0.12-nikolaik" \
  -v /var/run/docker.sock:/var/run/docker.sock \
  -p 3000:3000 \
  --name openhands-app \
  docker.all-hands.dev/all-hands-ai/openhands:0.12

# 6. (اختياري) إذا احتجت تثبيت jq في أي بيئة أو حاوية
sudo apt update && sudo apt install jq

# 7. (اختياري) سكربت Bash يعرض أعلى قصة في Hacker News
cat << 'EOF' > top_hn_story.sh
#!/bin/bash
TOP_ID=$(curl -s https://hacker-news.firebaseio.com/v0/topstories.json | jq '.[0]')
curl -s "https://hacker-news.firebaseio.com/v0/item/$TOP_ID.json" | jq '{title, url, score, by}'
EOF
chmod +x top_hn_story.sh
./top_hn_story.sh

# 8. بعد تشغيل OpenHands، افتح المتصفح على:
http://localhost:3000

# 9. إعدادات OpenHands (من المتصفح):
# - Base URL: http://172.21.151.42:11434
# - Model: llama3:latest
# - API Key: (اتركها فارغ أو اكتب ollama إذا كان إجباري)
# ثم اضغط Save





















############################################################



























1. إعدادات OpenHands من المتصفح
في واجهة OpenHands (Settings > AI Provider Configuration):
Base URL:
http://172.21.151.42:11434
Model:
llama3:latest
API Key:
(اتركه فارغ أو اكتب ollama)
بعد التعديل، اضغط Save وأعد تجربة النموذج.
2. تأكد من تزامن متغيرات البيئة
لو في إعدادات OpenHands أي خانة فيها 127.0.0.1 أو /v1، غيّرها إلى
http://172.21.151.42:11434
لا تضع /v1 في العنوان.







docker run -it --privileged --rm \
  -e LLM_API_KEY="ollama" \
  -e OLLAMA_HOST="http://172.21.151.42:11434" \
  -e LLM_OLLAMA_BASE_URL="http://172.21.151.42:11434" \
  -e OLLAMA_BASE_URL="http://172.21.151.42:11434" \
  -e OLLAMA_URL="http://172.21.151.42:11434" \
  -e SANDBOX_RUNTIME_CONTAINER_IMAGE="docker.all-hands.dev/all-hands-ai/runtime:0.12-nikolaik" \
  -v /var/run/docker.sock:/var/run/docker.sock \
  -p 3000:3000 \
  --name openhands-app \
  docker.all-hands.dev/all-hands-ai/openhands:0.12





m@DESKTOP-MUB16F8:~$ curl http://127.0.0.1:11434/api/tags
url http://<IP الخاص بك>:11434/api/tags{"models":[]}m@DESKTOP-MUB16F8:~$ curl http://<IP الخاص بك>:11434/api/tags
-bash: IP: No such file or directory
m@DESKTOP-MUB16F8:~$ OLLAMA_HOST=0.0.0.0 ollama serve
Error: listen tcp 0.0.0.0:11434: bind: address already in use
m@DESKTOP-MUB16F8:~$ curl http://127.0.0.1:11434/api/tags
://172.21.151.42:11434/api/tags{"models":[]}m@DESKTOP-MUB16F8:~$ curl http://172.21.151.42:11434/api/tags
{"models":[]}m@DESKTOP-MUB16F8:~$ curl http://127.0.0.1:11434/api/tags
{"models":[]}m@DESKTOP-MUB16F8:~$
m@DESKTOP-MUB16F8:~$
m@DESKTOP-MUB16F8:~$
m@DESKTOP-MUB16F8:~$ curl http://127.0.0.1:11434/api/tags
{"models":[]}m@DESKTOP-MUB16F8:~$ curl http://172.21.151.42:11434/api/tags
{"models":[]}m@DESKTOP-MUB16F8:~$
m@DESKTOP-MUB16F8:~$
m@DESKTOP-MUB16F8:~$ curl http://172.21.151.42:11434/api/tags
{"models":[]}m@DESKTOP-MUB16F8:~$ ollama list
NAME    ID    SIZE    MODIFIED
m@DESKTOP-MUB16F8:~$ ollama pull llama3
pulling manifest
pulling 6a0746a1ec1a: 100% ▕██████████████████████████████████████████████████████████▏ 4.7 GB
pulling 4fa551d4f938: 100% ▕██████████████████████████████████████████████████████████▏  12 KB
pulling 8ab4849b038c: 100% ▕██████████████████████████████████████████████████████████▏  254 B
pulling 577073ffcc6c: 100% ▕██████████████████████████████████████████████████████████▏  110 B
pulling 3f8eb4da87fa: 100% ▕██████████████████████████████████████████████████████████▏  485 B
verifying sha256 digest
writing manifest
success
m@DESKTOP-MUB16F8:~$ ollama list
NAME             ID              SIZE      MODIFIED
llama3:latest    365c0bd3c000    4.7 GB    48 seconds ago
m@DESKTOP-MUB16F8:~$ docker run -it --rm \
-e OLLAM>   -e OLLAMA_HOST="http://172.21.151.42:11434" \
>   -e LLM_OLLAMA_BASE_URL="http://172.21.151.42:11434" \
>   -e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.12-nikolaik \
 /var/>   -v /var/run/docker.sock:/var/run/docker.sock \
>   -p 3000:3000 \
>   --add-host host.docker.internal:host-gateway \
>   --name openhands-app \
>   docker.all-hands.dev/all-hands-ai/openhands:0.12
Starting OpenHands...
Running OpenHands as root
INFO:     Started server process [10]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:3000 (Press CTRL+C to quit)
INFO:     172.17.0.1:38464 - "GET / HTTP/1.1" 200 OK
INFO:     172.17.0.1:38464 - "GET /locales/en/translation.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:38464 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:38472 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:38472 - "GET /config.json HTTP/1.1" 200 OK
21:23:20 - openhands:INFO: github.py:14 - Initializing UserVerifier
21:23:20 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
21:23:20 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
21:23:20 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     ('172.17.0.1', 38484) - "WebSocket /ws" [accepted]
21:23:20 - openhands:ERROR: auth.py:27 - Invalid token
INFO:     connection open
INFO:     172.17.0.1:38472 - "GET /api/options/models HTTP/1.1" 200 OK
INFO:     172.17.0.1:38464 - "GET /api/options/agents HTTP/1.1" 200 OK
INFO:     172.17.0.1:38488 - "GET /api/options/security-analyzers HTTP/1.1" 200 OK
INFO:     connection closed
INFO:     172.17.0.1:38488 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:38472 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:45728 - "GET / HTTP/1.1" 200 OK
INFO:     172.17.0.1:45728 - "GET /favicon.ico HTTP/1.1" 200 OK
INFO:     172.17.0.1:45728 - "GET /locales/en/translation.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:45728 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:45732 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:45728 - "GET /api/options/agents HTTP/1.1" 200 OK
INFO:     172.17.0.1:45732 - "GET /api/options/models HTTP/1.1" 200 OK
INFO:     172.17.0.1:45748 - "GET /api/options/security-analyzers HTTP/1.1" 200 OK
INFO:     172.17.0.1:39856 - "GET /assets/settings-B80uBrgv.js HTTP/1.1" 200 OK
INFO:     172.17.0.1:39856 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:39856 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:39856 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:39862 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:39862 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:39856 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:59718 - "GET /assets/_oh.app-BbuKviuw.js HTTP/1.1" 200 OK
INFO:     172.17.0.1:59730 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:59722 - "GET /assets/route-CCYv0vF5.js HTTP/1.1" 200 OK
INFO:     172.17.0.1:59718 - "GET /assets/_oh.app-o9tOh4U3.js HTTP/1.1" 200 OK
INFO:     172.17.0.1:59730 - "GET /assets/index-BXT04Q8N.js HTTP/1.1" 200 OK
INFO:     172.17.0.1:59744 - "GET /assets/clear-session-Bbh6tTiy.js HTTP/1.1" 200 OK
INFO:     172.17.0.1:59734 - "GET /assets/declaration-DdUOQw3g.js HTTP/1.1" 200 OK
INFO:     172.17.0.1:59740 - "GET /assets/useScrollToBottom-B8_iP14x.js HTTP/1.1" 200 OK
INFO:     172.17.0.1:59722 - "GET /assets/index-C4MRHScb.js HTTP/1.1" 200 OK
INFO:     172.17.0.1:59730 - "GET /assets/index-wBZoZNP4.js HTTP/1.1" 200 OK
INFO:     172.17.0.1:59718 - "GET /assets/Terminal-DIsziyk5.js HTTP/1.1" 200 OK
INFO:     172.17.0.1:59730 - "GET /assets/Terminal-CGrrDQr5.css HTTP/1.1" 200 OK
21:24:36 - openhands:INFO: github.py:14 - Initializing UserVerifier
21:24:36 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
21:24:36 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
21:24:36 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     ('172.17.0.1', 59758) - "WebSocket /ws" [accepted]
21:24:36 - openhands:INFO: listen.py:336 - New session: b2d909c7-e4d1-4858-9f41-e3be86a78fb6
INFO:     connection open
21:24:36 - openhands:WARNING: codeact_agent.py:101 - Function calling not supported for model ollama/llama3:latest. Disabling function calling.
INFO:     172.17.0.1:59718 - "GET /config.json HTTP/1.1" 200 OK
21:24:36 - openhands:INFO: base.py:98 - [runtime b2d909c7-e4d1-4858-9f41-e3be86a78fb6] Starting runtime with image: docker.all-hands.dev/all-hands-ai/runtime:0.12-nikolaik
21:24:37 - openhands:INFO: base.py:98 - [runtime b2d909c7-e4d1-4858-9f41-e3be86a78fb6] Container started: openhands-runtime-b2d909c7-e4d1-4858-9f41-e3be86a78fb6
21:24:37 - openhands:INFO: base.py:98 - [runtime b2d909c7-e4d1-4858-9f41-e3be86a78fb6] Waiting for client to become ready at http://host.docker.internal:36771...
21:25:29 - openhands:INFO: base.py:98 - [runtime b2d909c7-e4d1-4858-9f41-e3be86a78fb6] Runtime is ready.
21:25:29 - openhands:WARNING: state.py:118 - Failed to restore state from session: sessions/b2d909c7-e4d1-4858-9f41-e3be86a78fb6/agent_state.pkl
21:25:29 - openhands:INFO: agent_controller.py:135 - [Agent Controller b2d909c7-e4d1-4858-9f41-e3be86a78fb6] Starting step loop...
21:25:29 - openhands:INFO: agent_controller.py:135 - [Agent Controller b2d909c7-e4d1-4858-9f41-e3be86a78fb6] Setting agent(CodeActAgent) state from AgentState.LOADING to AgentState.INIT
21:25:29 - openhands:INFO: agent_controller.py:135 - [Agent Controller b2d909c7-e4d1-4858-9f41-e3be86a78fb6] Setting agent(CodeActAgent) state from AgentState.INIT to AgentState.RUNNING
21:25:29 - openhands:INFO: github.py:14 - Initializing UserVerifier
21:25:29 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
21:25:29 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
21:25:29 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     172.17.0.1:58772 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:58786 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:58760 - "GET /api/list-files HTTP/1.1" 200 OK
21:25:29 - openhands:INFO: github.py:14 - Initializing UserVerifier
21:25:29 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
21:25:29 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
21:25:29 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     172.17.0.1:58786 - "GET /api/list-files HTTP/1.1" 200 OK
21:25:29 - openhands:INFO: github.py:14 - Initializing UserVerifier
21:25:29 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
21:25:29 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
21:25:29 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     172.17.0.1:58760 - "GET /api/list-files HTTP/1.1" 200 OK


==============
[Agent Controller b2d909c7-e4d1-4858-9f41-e3be86a78fb6] LEVEL 0 LOCAL STEP 0 GLOBAL STEP 0


Provider List: https://docs.litellm.ai/docs/providers


Provider List: https://docs.litellm.ai/docs/providers


Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

21:25:30 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd337ac5190>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #1 | You can customize retry values in the configuration.

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

21:25:48 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd337ab93d0>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #2 | You can customize retry values in the configuration.

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

21:26:03 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd33794f9b0>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #3 | You can customize retry values in the configuration.

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

21:26:20 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd337964e00>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #4 | You can customize retry values in the configuration.

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

21:26:36 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd33794f680>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #5 | You can customize retry values in the configuration.

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

21:27:11 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd337964530>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #6 | You can customize retry values in the configuration.

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

21:28:19 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd337ac6c90>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #7 | You can customize retry values in the configuration.
^CINFO:     Shutting down
21:30:23 - openhands:INFO: session.py:66 - WebSocket disconnected, sid: b2d909c7-e4d1-4858-9f41-e3be86a78fb6
INFO:     connection closed
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [10]
^C^CException ignored in: <module 'threading' from '/usr/local/lib/python3.12/threading.py'>
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/threading.py", line 1592, in _shutdown
    atexit_call()
  File "/usr/local/lib/python3.12/concurrent/futures/thread.py", line 31, in _python_exit
    t.join()
  File "/usr/local/lib/python3.12/threading.py", line 1147, in join
    self._wait_for_tstate_lock()
  File "/usr/local/lib/python3.12/threading.py", line 1167, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt:

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 199, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 789, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 495, in _make_request
    conn.request(
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 441, in request
    self.endheaders()
  File "/usr/local/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/local/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/local/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 279, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 214, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7fd3381e37a0>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 843, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd3381e37a0>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/litellm/main.py", line 2709, in completion
    generator = ollama.get_ollama_response(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/llms/ollama.py", line 316, in get_ollama_response
    response = requests.post(
               ^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/adapters.py", line 700, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd3381e37a0>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/openhands/controller/agent_controller.py", line 168, in start_step_loop
    await self._step()
  File "/app/openhands/controller/agent_controller.py", line 464, in _step
    action = self.agent.step(self.state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/agenthub/codeact_agent/codeact_agent.py", line 359, in step
    response = self.llm.completion(**params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 336, in wrapped_f
    return copy(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 475, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 478, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/llm/llm.py", line 196, in wrapper
    resp: ModelResponse = completion_unwrapped(*args, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/utils.py", line 1013, in wrapper
    raise e
  File "/app/.venv/lib/python3.12/site-packages/litellm/utils.py", line 903, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/main.py", line 2999, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2116, in exception_type
    raise e
  File "/app/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 1785, in exception_type
    raise ServiceUnavailableError(
litellm.exceptions.ServiceUnavailableError: litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd3381e37a0>: Failed to establish a new connection: [Errno 111] Connection refused'))
21:30:27 - openhands:ERROR: agent_controller.py:135 - [Agent Controller b2d909c7-e4d1-4858-9f41-e3be86a78fb6] Error while running the agent: litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd3381e37a0>: Failed to establish a new connection: [Errno 111] Connection refused'))
21:30:27 - openhands:ERROR: agent_controller.py:135 - [Agent Controller b2d909c7-e4d1-4858-9f41-e3be86a78fb6] Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 199, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 789, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 495, in _make_request
    conn.request(
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 441, in request
    self.endheaders()
  File "/usr/local/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/local/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/local/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 279, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 214, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7fd3381e37a0>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 843, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd3381e37a0>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/litellm/main.py", line 2709, in completion
    generator = ollama.get_ollama_response(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/llms/ollama.py", line 316, in get_ollama_response
    response = requests.post(
               ^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/adapters.py", line 700, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd3381e37a0>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/openhands/controller/agent_controller.py", line 168, in start_step_loop
    await self._step()
  File "/app/openhands/controller/agent_controller.py", line 464, in _step
    action = self.agent.step(self.state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/agenthub/codeact_agent/codeact_agent.py", line 359, in step
    response = self.llm.completion(**params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 336, in wrapped_f
    return copy(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 475, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 478, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/llm/llm.py", line 196, in wrapper
    resp: ModelResponse = completion_unwrapped(*args, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/utils.py", line 1013, in wrapper
    raise e
  File "/app/.venv/lib/python3.12/site-packages/litellm/utils.py", line 903, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/main.py", line 2999, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2116, in exception_type
    raise e
  File "/app/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 1785, in exception_type
    raise ServiceUnavailableError(
litellm.exceptions.ServiceUnavailableError: litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd3381e37a0>: Failed to establish a new connection: [Errno 111] Connection refused'))

21:30:27 - openhands:INFO: agent_controller.py:135 - [Agent Controller b2d909c7-e4d1-4858-9f41-e3be86a78fb6] Setting agent(CodeActAgent) state from AgentState.RUNNING to AgentState.ERROR
m@DESKTOP-MUB16F8:~$ docker run -it --rm \
-e OLL>   -e OLLAMA_HOST="http://172.21.151.42:11434" \
>   -e LLM_OLLAMA_BASE_URL="http://172.21.151.42:11434" \
>   -e OLLAMA_BASE_URL="http://172.21.151.42:11434" \
>   -e OLLAMA_URL="http://172.21.151.42:11434" \
>   -e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.12-nikolaik \
>   -v /var/run/docker.sock:/var/run/docker.sock \
>   -p 3000:3000 \
>   --add-host host.docker.internal:host-gateway \
>   --name openhands-app \
>   docker.all-hands.dev/all-hands-ai/openhands:0.12
Starting OpenHands...
Running OpenHands as root
INFO:     Started server process [10]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:3000 (Press CTRL+C to quit)
INFO:     172.17.0.1:46008 - "GET / HTTP/1.1" 200 OK
INFO:     172.17.0.1:46008 - "GET /locales/en/translation.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:46008 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:46014 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:46014 - "GET /config.json HTTP/1.1" 200 OK
21:31:18 - openhands:INFO: github.py:14 - Initializing UserVerifier
21:31:18 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
21:31:18 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
21:31:18 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     ('172.17.0.1', 46024) - "WebSocket /ws" [accepted]
21:31:18 - openhands:ERROR: auth.py:27 - Invalid token
INFO:     172.17.0.1:46014 - "GET /api/options/models HTTP/1.1" 200 OK
INFO:     connection open
INFO:     172.17.0.1:46008 - "GET /api/options/agents HTTP/1.1" 200 OK
INFO:     172.17.0.1:46030 - "GET /api/options/security-analyzers HTTP/1.1" 200 OK
INFO:     connection closed
INFO:     172.17.0.1:46030 - "GET /assets/end-session-CpBsKLW_.js HTTP/1.1" 200 OK
INFO:     172.17.0.1:46030 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:46008 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:42668 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:42674 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:42686 - "GET /config.json HTTP/1.1" 200 OK
21:32:56 - openhands:INFO: github.py:14 - Initializing UserVerifier
21:32:56 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
21:32:56 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
21:32:56 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     ('172.17.0.1', 42688) - "WebSocket /ws" [accepted]
21:32:56 - openhands:INFO: listen.py:336 - New session: 844b5dbd-fb1d-4b36-82e1-5b4a6215601c
INFO:     connection open
21:32:56 - openhands:WARNING: codeact_agent.py:101 - Function calling not supported for model ollama/llama3:latest. Disabling function calling.
INFO:     172.17.0.1:42686 - "GET /config.json HTTP/1.1" 200 OK
21:32:56 - openhands:INFO: base.py:98 - [runtime 844b5dbd-fb1d-4b36-82e1-5b4a6215601c] Starting runtime with image: docker.all-hands.dev/all-hands-ai/runtime:0.12-nikolaik
21:32:57 - openhands:INFO: base.py:98 - [runtime 844b5dbd-fb1d-4b36-82e1-5b4a6215601c] Container started: openhands-runtime-844b5dbd-fb1d-4b36-82e1-5b4a6215601c
21:32:57 - openhands:INFO: base.py:98 - [runtime 844b5dbd-fb1d-4b36-82e1-5b4a6215601c] Waiting for client to become ready at http://host.docker.internal:36650...
21:33:50 - openhands:INFO: base.py:98 - [runtime 844b5dbd-fb1d-4b36-82e1-5b4a6215601c] Runtime is ready.
21:33:50 - openhands:WARNING: state.py:118 - Failed to restore state from session: sessions/844b5dbd-fb1d-4b36-82e1-5b4a6215601c/agent_state.pkl
21:33:50 - openhands:INFO: agent_controller.py:135 - [Agent Controller 844b5dbd-fb1d-4b36-82e1-5b4a6215601c] Starting step loop...
21:33:51 - openhands:INFO: agent_controller.py:135 - [Agent Controller 844b5dbd-fb1d-4b36-82e1-5b4a6215601c] Setting agent(CodeActAgent) state from AgentState.LOADING to AgentState.INIT
21:33:51 - openhands:INFO: agent_controller.py:135 - [Agent Controller 844b5dbd-fb1d-4b36-82e1-5b4a6215601c] Setting agent(CodeActAgent) state from AgentState.INIT to AgentState.RUNNING
21:33:51 - openhands:INFO: github.py:14 - Initializing UserVerifier
21:33:51 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
21:33:51 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
21:33:51 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     172.17.0.1:42910 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:42918 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:42894 - "GET /api/list-files HTTP/1.1" 200 OK
21:33:51 - openhands:INFO: github.py:14 - Initializing UserVerifier
21:33:51 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
21:33:51 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
21:33:51 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     172.17.0.1:42918 - "GET /api/list-files HTTP/1.1" 200 OK
21:33:51 - openhands:INFO: github.py:14 - Initializing UserVerifier
21:33:51 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
21:33:51 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
21:33:51 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     172.17.0.1:42894 - "GET /api/list-files HTTP/1.1" 200 OK


==============
[Agent Controller 844b5dbd-fb1d-4b36-82e1-5b4a6215601c] LEVEL 0 LOCAL STEP 0 GLOBAL STEP 0


Provider List: https://docs.litellm.ai/docs/providers


Provider List: https://docs.litellm.ai/docs/providers


Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

21:33:52 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f56e8c739e0>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #1 | You can customize retry values in the configuration.

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

21:34:07 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f56e8c719d0>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #2 | You can customize retry values in the configuration.
^CINFO:     Shutting down
21:34:22 - openhands:INFO: session.py:66 - WebSocket disconnected, sid: 844b5dbd-fb1d-4b36-82e1-5b4a6215601c
INFO:     connection closed
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [10]
^C^CException ignored in: <module 'threading' from '/usr/local/lib/python3.12/threading.py'>
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/threading.py", line 1592, in _shutdown
    atexit_call()
  File "/usr/local/lib/python3.12/concurrent/futures/thread.py", line 31, in _python_exit
    t.join()
  File "/usr/local/lib/python3.12/threading.py", line 1147, in join
    self._wait_for_tstate_lock()
  File "/usr/local/lib/python3.12/threading.py", line 1167, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt:

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 199, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 789, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 495, in _make_request
    conn.request(
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 441, in request
    self.endheaders()
  File "/usr/local/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/local/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/local/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 279, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 214, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f56e8c607d0>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 843, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f56e8c607d0>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/litellm/main.py", line 2709, in completion
    generator = ollama.get_ollama_response(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/llms/ollama.py", line 316, in get_ollama_response
    response = requests.post(
               ^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/adapters.py", line 700, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f56e8c607d0>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/openhands/controller/agent_controller.py", line 168, in start_step_loop
    await self._step()
  File "/app/openhands/controller/agent_controller.py", line 464, in _step
    action = self.agent.step(self.state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/agenthub/codeact_agent/codeact_agent.py", line 359, in step
    response = self.llm.completion(**params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 336, in wrapped_f
    return copy(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 475, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 478, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/llm/llm.py", line 196, in wrapper
    resp: ModelResponse = completion_unwrapped(*args, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/utils.py", line 1013, in wrapper
    raise e
  File "/app/.venv/lib/python3.12/site-packages/litellm/utils.py", line 903, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/main.py", line 2999, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2116, in exception_type
    raise e
  File "/app/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 1785, in exception_type
    raise ServiceUnavailableError(
litellm.exceptions.ServiceUnavailableError: litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f56e8c607d0>: Failed to establish a new connection: [Errno 111] Connection refused'))
21:34:24 - openhands:ERROR: agent_controller.py:135 - [Agent Controller 844b5dbd-fb1d-4b36-82e1-5b4a6215601c] Error while running the agent: litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f56e8c607d0>: Failed to establish a new connection: [Errno 111] Connection refused'))
21:34:24 - openhands:ERROR: agent_controller.py:135 - [Agent Controller 844b5dbd-fb1d-4b36-82e1-5b4a6215601c] Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 199, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 789, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 495, in _make_request
    conn.request(
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 441, in request
    self.endheaders()
  File "/usr/local/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/local/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/local/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 279, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 214, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f56e8c607d0>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 843, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f56e8c607d0>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/litellm/main.py", line 2709, in completion
    generator = ollama.get_ollama_response(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/llms/ollama.py", line 316, in get_ollama_response
    response = requests.post(
               ^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/adapters.py", line 700, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f56e8c607d0>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/openhands/controller/agent_controller.py", line 168, in start_step_loop
    await self._step()
  File "/app/openhands/controller/agent_controller.py", line 464, in _step
    action = self.agent.step(self.state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/agenthub/codeact_agent/codeact_agent.py", line 359, in step
    response = self.llm.completion(**params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 336, in wrapped_f
    return copy(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 475, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 478, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/llm/llm.py", line 196, in wrapper
    resp: ModelResponse = completion_unwrapped(*args, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/utils.py", line 1013, in wrapper
    raise e
  File "/app/.venv/lib/python3.12/site-packages/litellm/utils.py", line 903, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/main.py", line 2999, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2116, in exception_type
    raise e
  File "/app/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 1785, in exception_type
    raise ServiceUnavailableError(
litellm.exceptions.ServiceUnavailableError: litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f56e8c607d0>: Failed to establish a new connection: [Errno 111] Connection refused'))

21:34:24 - openhands:INFO: agent_controller.py:135 - [Agent Controller 844b5dbd-fb1d-4b36-82e1-5b4a6215601c] Setting agent(CodeActAgent) state from AgentState.RUNNING to AgentState.ERROR
m@DESKTOP-MUB16F8:~$
cker runm@DESKTOP-MUB16F8:~$ sudo docker run -it --privileged --rm -ways \al
>   -e LLM_API_KEY="ollama" \
>   -e OLLAMA_HOST="http://host.docker.internal:11434"
 LLM_OLLAMA_BASE_URL="http://host.docker.internal:11434"
  -e OLLAMA_BASE_URL="http://host.docker.internal:11434"
  -e OLLAMA_URL="http://host.docker.internal:11434"
  -e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.12-nikolaik \
  -v /var/run/docker.sock:/var/run/docker.sock \
  -p 3000:3000 \
  --add-host host.docker.internal:host-gateway \
  --name openhands-app \
  docker.all-hands.dev/all-hands-ai/openhands:0.12[sudo] password for m:
Sorry, try again.
[sudo] password for m:
docker: 'docker run' requires at least 1 argument

Usage:  docker run [OPTIONS] IMAGE [COMMAND] [ARG...]

See 'docker run --help' for more information
m@DESKTOP-MUB16F8:~$ docker run -it --rm \
>   -e OLLAMA_HOST="http://host.docker.internal:11434" \
>   -e LLM_OLLAMA_BASE_URL="http://host.docker.internal:11434" \
>   -e OLLAMA_BASE_URL="http://host.docker.internal:11434" \
>   -e OLLAMA_URL="http://host.docker.internal:11434" \
>   -e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.12-nikolaik \
>   -v /var/run/docker.sock:/var/run/docker.sock \
>   -p 3000:3000 \
>   --add-host host.docker.internal:host-gateway \
>   --name openhands-app \
>   docker.all-hands.dev/all-hands-ai/openhands:0.12
Starting OpenHands...
Running OpenHands as root
INFO:     Started server process [10]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:3000 (Press CTRL+C to quit)
INFO:     172.17.0.1:57364 - "GET / HTTP/1.1" 200 OK
INFO:     172.17.0.1:57364 - "GET /locales/en/translation.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:57364 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:57376 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:57376 - "GET /config.json HTTP/1.1" 200 OK
21:37:08 - openhands:ERROR: listen.py:413 - Error getting OLLAMA models: HTTPConnectionPool(host='host.docker.internal', port=11434): Max retries exceeded with url: /api/tags (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9448158500>: Failed to establish a new connection: [Errno 111] Connection refused'))
Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 199, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 789, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 495, in _make_request
    conn.request(
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 441, in request
    self.endheaders()
  File "/usr/local/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/local/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/local/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 279, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 214, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f9448158500>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 843, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='host.docker.internal', port=11434): Max retries exceeded with url: /api/tags (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9448158500>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/openhands/server/listen.py", line 406, in get_litellm_models
    ollama_models_list = requests.get(ollama_url, timeout=3).json()[                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/adapters.py", line 700, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='host.docker.internal', port=11434): Max retries exceeded with url: /api/tags (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9448158500>: Failed to establish a new connection: [Errno 111] Connection refused'))
INFO:     172.17.0.1:57376 - "GET /api/options/models HTTP/1.1" 200 OK
21:37:08 - openhands:INFO: github.py:14 - Initializing UserVerifier
21:37:08 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
21:37:08 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
21:37:08 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     ('172.17.0.1', 57386) - "WebSocket /ws" [accepted]
21:37:08 - openhands:ERROR: auth.py:27 - Invalid token
INFO:     connection open
INFO:     172.17.0.1:57364 - "GET /api/options/agents HTTP/1.1" 200 OK
INFO:     172.17.0.1:57388 - "GET /api/options/security-analyzers HTTP/1.1" 200 OK
INFO:     connection closed
INFO:     172.17.0.1:57388 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:57364 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:56212 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:56214 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:56214 - "GET /config.json HTTP/1.1" 200 OK
21:37:32 - openhands:INFO: github.py:14 - Initializing UserVerifier
21:37:32 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
21:37:32 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
21:37:32 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     ('172.17.0.1', 56220) - "WebSocket /ws" [accepted]
21:37:32 - openhands:INFO: listen.py:336 - New session: ec6f817b-3465-47bc-8889-de162172081a
INFO:     connection open
21:37:33 - openhands:WARNING: codeact_agent.py:101 - Function calling not supported for model ollama/llama3:latest. Disabling function calling.
INFO:     172.17.0.1:56214 - "GET /config.json HTTP/1.1" 200 OK
21:37:33 - openhands:INFO: base.py:98 - [runtime ec6f817b-3465-47bc-8889-de162172081a] Starting runtime with image: docker.all-hands.dev/all-hands-ai/runtime:0.12-nikolaik
21:37:33 - openhands:INFO: base.py:98 - [runtime ec6f817b-3465-47bc-8889-de162172081a] Container started: openhands-runtime-ec6f817b-3465-47bc-8889-de162172081a
21:37:33 - openhands:INFO: base.py:98 - [runtime ec6f817b-3465-47bc-8889-de162172081a] Waiting for client to become ready at http://host.docker.internal:39018...
21:38:05 - openhands:INFO: base.py:98 - [runtime ec6f817b-3465-47bc-8889-de162172081a] Runtime is ready.
21:38:05 - openhands:WARNING: state.py:118 - Failed to restore state from session: sessions/ec6f817b-3465-47bc-8889-de162172081a/agent_state.pkl
21:38:05 - openhands:INFO: agent_controller.py:135 - [Agent Controller ec6f817b-3465-47bc-8889-de162172081a] Starting step loop...
21:38:05 - openhands:INFO: agent_controller.py:135 - [Agent Controller ec6f817b-3465-47bc-8889-de162172081a] Setting agent(CodeActAgent) state from AgentState.LOADING to AgentState.INIT
21:38:05 - openhands:INFO: agent_controller.py:135 - [Agent Controller ec6f817b-3465-47bc-8889-de162172081a] Setting agent(CodeActAgent) state from AgentState.INIT to AgentState.RUNNING
21:38:05 - openhands:INFO: github.py:14 - Initializing UserVerifier
21:38:05 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
21:38:05 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
21:38:05 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     172.17.0.1:41574 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:41590 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:41558 - "GET /api/list-files HTTP/1.1" 200 OK
21:38:06 - openhands:INFO: github.py:14 - Initializing UserVerifier
21:38:06 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
21:38:06 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
21:38:06 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     172.17.0.1:41590 - "GET /api/list-files HTTP/1.1" 200 OK
21:38:06 - openhands:INFO: github.py:14 - Initializing UserVerifier
21:38:06 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
21:38:06 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
21:38:06 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     172.17.0.1:41558 - "GET /api/list-files HTTP/1.1" 200 OK


==============
[Agent Controller ec6f817b-3465-47bc-8889-de162172081a] LEVEL 0 LOCAL STEP 0 GLOBAL STEP 0


Provider List: https://docs.litellm.ai/docs/providers


Provider List: https://docs.litellm.ai/docs/providers


Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

21:38:07 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f944777f6b0>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #1 | You can customize retry values in the configuration.

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

21:38:22 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f944761a1b0>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #2 | You can customize retry values in the configuration.

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

21:38:38 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f944761aae0>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #3 | You can customize retry values in the configuration.

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

21:38:53 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f944761c0b0>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #4 | You can customize retry values in the configuration.

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

21:39:11 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f944761abd0>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #5 | You can customize retry values in the configuration.

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

21:39:46 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f944761d4f0>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #6 | You can customize retry values in the configuration.

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

21:40:54 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9447e8cd70>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #7 | You can customize retry values in the configuration.

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 199, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 789, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 495, in _make_request
    conn.request(
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 441, in request
    self.endheaders()
  File "/usr/local/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/local/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/local/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 279, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 214, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f944761dee0>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 843, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f944761dee0>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/litellm/main.py", line 2709, in completion
    generator = ollama.get_ollama_response(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/llms/ollama.py", line 316, in get_ollama_response
    response = requests.post(
               ^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/adapters.py", line 700, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f944761dee0>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/openhands/controller/agent_controller.py", line 168, in start_step_loop
    await self._step()
  File "/app/openhands/controller/agent_controller.py", line 464, in _step
    action = self.agent.step(self.state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/agenthub/codeact_agent/codeact_agent.py", line 359, in step
    response = self.llm.completion(**params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 336, in wrapped_f
    return copy(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 475, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 478, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/llm/llm.py", line 196, in wrapper
    resp: ModelResponse = completion_unwrapped(*args, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/utils.py", line 1013, in wrapper
    raise e
  File "/app/.venv/lib/python3.12/site-packages/litellm/utils.py", line 903, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/main.py", line 2999, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2116, in exception_type
    raise e
  File "/app/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 1785, in exception_type
    raise ServiceUnavailableError(
litellm.exceptions.ServiceUnavailableError: litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f944761dee0>: Failed to establish a new connection: [Errno 111] Connection refused'))
21:43:01 - openhands:ERROR: agent_controller.py:135 - [Agent Controller ec6f817b-3465-47bc-8889-de162172081a] Error while running the agent: litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f944761dee0>: Failed to establish a new connection: [Errno 111] Connection refused'))
21:43:01 - openhands:ERROR: agent_controller.py:135 - [Agent Controller ec6f817b-3465-47bc-8889-de162172081a] Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 199, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 789, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 495, in _make_request
    conn.request(
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 441, in request
    self.endheaders()
  File "/usr/local/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/local/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/local/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 279, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 214, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f944761dee0>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 843, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f944761dee0>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/litellm/main.py", line 2709, in completion
    generator = ollama.get_ollama_response(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/llms/ollama.py", line 316, in get_ollama_response
    response = requests.post(
               ^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/adapters.py", line 700, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f944761dee0>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/openhands/controller/agent_controller.py", line 168, in start_step_loop
    await self._step()
  File "/app/openhands/controller/agent_controller.py", line 464, in _step
    action = self.agent.step(self.state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/agenthub/codeact_agent/codeact_agent.py", line 359, in step
    response = self.llm.completion(**params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 336, in wrapped_f
    return copy(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 475, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 478, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/llm/llm.py", line 196, in wrapper
    resp: ModelResponse = completion_unwrapped(*args, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/utils.py", line 1013, in wrapper
    raise e
  File "/app/.venv/lib/python3.12/site-packages/litellm/utils.py", line 903, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/main.py", line 2999, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2116, in exception_type
    raise e
  File "/app/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 1785, in exception_type
    raise ServiceUnavailableError(
litellm.exceptions.ServiceUnavailableError: litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f944761dee0>: Failed to establish a new connection: [Errno 111] Connection refused'))

21:43:01 - openhands:INFO: agent_controller.py:135 - [Agent Controller ec6f817b-3465-47bc-8889-de162172081a] Setting agent(CodeActAgent) state from AgentState.RUNNING to AgentState.ERROR
^CINFO:     Shutting down
21:45:11 - openhands:INFO: session.py:66 - WebSocket disconnected, sid: ec6f817b-3465-47bc-8889-de162172081a
INFO:     connection closed
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [10]
m@DESKTOP-MUB16F8:~$ docker run -it --privileged --rm \
>   -e LLM_API_KEY="ollama" \
>   -e OLLAMA_HOST="http://172.21.151.42:11434" \
e LLM_O>   -e LLM_OLLAMA_BASE_URL="http://172.21.151.42:11434" \
LAMA_BA>   -e OLLAMA_BASE_URL="http://172.21.151.42:11434" \
>   -e OLLAMA_URL="http://172.21.151.42:11434" \
>   -e SANDBOX_RUNTIME_CONTAINER_IMAGE="docker.all-hands.dev/all-hands-ai/runtime:0.12-nikolaik" \
>   -v /var/run/docker.sock:/var/run/docker.sock \
>   -p 3000:3000 \
>   --name openhands-app \
>   docker.all-hands.dev/all-hands-ai/openhands:0.12
Starting OpenHands...
Running OpenHands as root
INFO:     Started server process [10]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:3000 (Press CTRL+C to quit)
INFO:     172.17.0.1:36792 - "GET / HTTP/1.1" 200 OK
INFO:     172.17.0.1:36792 - "GET /locales/en/translation.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:36792 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:36804 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:36804 - "GET /config.json HTTP/1.1" 200 OK
21:47:40 - openhands:INFO: github.py:14 - Initializing UserVerifier
21:47:40 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
21:47:40 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
21:47:40 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     ('172.17.0.1', 36814) - "WebSocket /ws" [accepted]
21:47:40 - openhands:ERROR: auth.py:27 - Invalid token
INFO:     connection open
INFO:     172.17.0.1:36804 - "GET /api/options/models HTTP/1.1" 200 OK
INFO:     172.17.0.1:36792 - "GET /api/options/agents HTTP/1.1" 200 OK
INFO:     172.17.0.1:36824 - "GET /api/options/security-analyzers HTTP/1.1" 200 OK
INFO:     connection closed
INFO:     172.17.0.1:36824 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:36792 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:41722 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:41724 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:41724 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:41722 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:41722 - "GET /config.json HTTP/1.1" 200 OK
21:50:12 - openhands:INFO: github.py:14 - Initializing UserVerifier
21:50:12 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
21:50:12 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
21:50:12 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     ('172.17.0.1', 36342) - "WebSocket /ws" [accepted]
21:50:12 - openhands:INFO: listen.py:336 - New session: c08adb0a-d9b2-4563-866f-0140cc503d3e
INFO:     connection open
21:50:12 - openhands:WARNING: codeact_agent.py:101 - Function calling not supported for model ollama/llama3:latest. Disabling function calling.
INFO:     172.17.0.1:41722 - "GET /config.json HTTP/1.1" 200 OK
21:50:13 - openhands:INFO: base.py:98 - [runtime c08adb0a-d9b2-4563-866f-0140cc503d3e] Starting runtime with image: docker.all-hands.dev/all-hands-ai/runtime:0.12-nikolaik
21:50:13 - openhands:INFO: base.py:98 - [runtime c08adb0a-d9b2-4563-866f-0140cc503d3e] Container started: openhands-runtime-c08adb0a-d9b2-4563-866f-0140cc503d3e
21:50:13 - openhands:INFO: base.py:98 - [runtime c08adb0a-d9b2-4563-866f-0140cc503d3e] Waiting for client to become ready at http://host.docker.internal:38003...
21:51:05 - openhands:INFO: base.py:98 - [runtime c08adb0a-d9b2-4563-866f-0140cc503d3e] Runtime is ready.
21:51:05 - openhands:WARNING: state.py:118 - Failed to restore state from session: sessions/c08adb0a-d9b2-4563-866f-0140cc503d3e/agent_state.pkl
21:51:05 - openhands:INFO: agent_controller.py:135 - [Agent Controller c08adb0a-d9b2-4563-866f-0140cc503d3e] Starting step loop...
21:51:05 - openhands:INFO: agent_controller.py:135 - [Agent Controller c08adb0a-d9b2-4563-866f-0140cc503d3e] Setting agent(CodeActAgent) state from AgentState.LOADING to AgentState.INIT
21:51:05 - openhands:INFO: agent_controller.py:135 - [Agent Controller c08adb0a-d9b2-4563-866f-0140cc503d3e] Setting agent(CodeActAgent) state from AgentState.INIT to AgentState.RUNNING
21:51:05 - openhands:INFO: github.py:14 - Initializing UserVerifier
21:51:05 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
21:51:05 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
21:51:05 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     172.17.0.1:43552 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:43554 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:43546 - "GET /api/list-files HTTP/1.1" 200 OK
21:51:05 - openhands:INFO: github.py:14 - Initializing UserVerifier
21:51:05 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
21:51:05 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
21:51:05 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     172.17.0.1:43554 - "GET /api/list-files HTTP/1.1" 200 OK
21:51:06 - openhands:INFO: github.py:14 - Initializing UserVerifier
21:51:06 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
21:51:06 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
21:51:06 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     172.17.0.1:43546 - "GET /api/list-files HTTP/1.1" 200 OK


==============
[Agent Controller c08adb0a-d9b2-4563-866f-0140cc503d3e] LEVEL 0 LOCAL STEP 0 GLOBAL STEP 0


Provider List: https://docs.litellm.ai/docs/providers


Provider List: https://docs.litellm.ai/docs/providers


Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

21:51:06 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f973248b380>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #1 | You can customize retry values in the configuration.

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

21:51:23 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9732b8ba40>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #2 | You can customize retry values in the configuration.

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

21:51:38 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9732328140>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #3 | You can customize retry values in the configuration.

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

21:51:55 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9732329580>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #4 | You can customize retry values in the configuration.

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

21:52:11 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9732513380>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #5 | You can customize retry values in the configuration.
INFO:     172.17.0.1:47786 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:47786 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:47796 - "GET /config.json HTTP/1.1" 200 OK
21:52:32 - openhands:INFO: session.py:66 - WebSocket disconnected, sid: c08adb0a-d9b2-4563-866f-0140cc503d3e
INFO:     connection closed
INFO:     172.17.0.1:47796 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:47796 - "GET /config.json HTTP/1.1" 200 OK
21:52:36 - openhands:INFO: github.py:14 - Initializing UserVerifier
21:52:36 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
21:52:36 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
21:52:36 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     ('172.17.0.1', 47808) - "WebSocket /ws" [accepted]
21:52:36 - openhands:INFO: listen.py:336 - New session: 7efbf261-c0d5-4ed3-82ae-144e1c9bea1b
INFO:     connection open
21:52:36 - openhands:WARNING: codeact_agent.py:101 - Function calling not supported for model ollama/llama3:latest. Disabling function calling.
INFO:     172.17.0.1:47796 - "GET /config.json HTTP/1.1" 200 OK
21:52:36 - openhands:INFO: base.py:98 - [runtime 7efbf261-c0d5-4ed3-82ae-144e1c9bea1b] Starting runtime with image: docker.all-hands.dev/all-hands-ai/runtime:0.12-nikolaik
21:52:37 - openhands:INFO: github.py:14 - Initializing UserVerifier
21:52:37 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
21:52:37 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
21:52:37 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     172.17.0.1:47796 - "GET /api/list-files HTTP/1.1" 500 Internal Server Error
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/docker/api/client.py", line 275, in _raise_for_status
    response.raise_for_status()
  File "/app/.venv/lib/python3.12/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: http+docker://localhost/v1.49/containers/openhands-runtime-7efbf261-c0d5-4ed3-82ae-144e1c9bea1b/json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 406, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/fastapi/applications.py", line 1054, in __call__
    await super().__call__(scope, receive, send)
  File "/app/.venv/lib/python3.12/site-packages/starlette/applications.py", line 113, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/app/.venv/lib/python3.12/site-packages/starlette/middleware/errors.py", line 187, in __call__
    raise exc
  File "/app/.venv/lib/python3.12/site-packages/starlette/middleware/errors.py", line 165, in __call__
    await self.app(scope, receive, _send)
  File "/app/.venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 185, in __call__
    with collapse_excgroups():
  File "/usr/local/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/app/.venv/lib/python3.12/site-packages/starlette/_utils.py", line 82, in collapse_excgroups
    raise exc
  File "/app/.venv/lib/python3.12/site-packages/starlette/middleware/base.py", line 187, in __call__
    response = await self.dispatch_func(request, call_next)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/server/listen.py", line 233, in attach_session
    request.state.conversation = await session_manager.attach_to_conversation(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/server/session/manager.py", line 53, in attach_to_conversation
    await c.connect()
  File "/app/openhands/server/session/conversation.py", line 42, in connect
    await self.runtime.connect()
  File "/app/openhands/runtime/impl/eventstream/eventstream_runtime.py", line 212, in connect
    self._attach_to_container()
  File "/app/openhands/runtime/impl/eventstream/eventstream_runtime.py", line 350, in _attach_to_container
    container = self.docker_client.containers.get(self.container_name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/docker/models/containers.py", line 954, in get
    resp = self.client.api.inspect_container(container_id)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/docker/utils/decorators.py", line 19, in wrapped
    return f(self, resource_id, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/docker/api/container.py", line 793, in inspect_container
    return self._result(
           ^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/docker/api/client.py", line 281, in _result
    self._raise_for_status(response)
  File "/app/.venv/lib/python3.12/site-packages/docker/api/client.py", line 277, in _raise_for_status
    raise create_api_error_from_http_exception(e) from e
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/docker/errors.py", line 39, in create_api_error_from_http_exception
    raise cls(e, response=response, explanation=explanation) from e
docker.errors.NotFound: 404 Client Error for http+docker://localhost/v1.49/containers/openhands-runtime-7efbf261-c0d5-4ed3-82ae-144e1c9bea1b/json: Not Found ("No such container: openhands-runtime-7efbf261-c0d5-4ed3-82ae-144e1c9bea1b")
21:52:37 - openhands:INFO: base.py:98 - [runtime 7efbf261-c0d5-4ed3-82ae-144e1c9bea1b] Container started: openhands-runtime-7efbf261-c0d5-4ed3-82ae-144e1c9bea1b
21:52:37 - openhands:INFO: base.py:98 - [runtime 7efbf261-c0d5-4ed3-82ae-144e1c9bea1b] Waiting for client to become ready at http://host.docker.internal:33243...
INFO:     172.17.0.1:44462 - "GET /app HTTP/1.1" 200 OK
21:52:41 - openhands:INFO: session.py:66 - WebSocket disconnected, sid: 7efbf261-c0d5-4ed3-82ae-144e1c9bea1b
INFO:     connection closed
INFO:     172.17.0.1:44462 - "GET /locales/en/translation.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:44462 - "GET /favicon.ico HTTP/1.1" 200 OK
INFO:     172.17.0.1:44462 - "GET /config.json HTTP/1.1" 200 OK
21:52:41 - openhands:INFO: github.py:14 - Initializing UserVerifier
21:52:41 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
21:52:41 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
21:52:41 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     172.17.0.1:44462 - "POST /api/authenticate HTTP/1.1" 200 OK
21:52:41 - openhands:INFO: github.py:14 - Initializing UserVerifier
21:52:41 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
21:52:41 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
21:52:41 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     ('172.17.0.1', 44484) - "WebSocket /ws" [accepted]
21:52:41 - openhands:INFO: listen.py:336 - New session: 7efbf261-c0d5-4ed3-82ae-144e1c9bea1b
INFO:     172.17.0.1:44462 - "GET /api/options/models HTTP/1.1" 200 OK
INFO:     connection open
INFO:     172.17.0.1:44468 - "GET /api/options/agents HTTP/1.1" 200 OK
INFO:     172.17.0.1:44494 - "GET /api/options/security-analyzers HTTP/1.1" 200 OK
21:52:41 - openhands:WARNING: codeact_agent.py:101 - Function calling not supported for model ollama/llama3:latest. Disabling function calling.
INFO:     172.17.0.1:44494 - "GET /config.json HTTP/1.1" 200 OK
21:52:42 - openhands:INFO: base.py:98 - [runtime 7efbf261-c0d5-4ed3-82ae-144e1c9bea1b] Starting runtime with image: docker.all-hands.dev/all-hands-ai/runtime:0.12-nikolaik
21:52:42 - openhands:ERROR: base.py:98 - [runtime 7efbf261-c0d5-4ed3-82ae-144e1c9bea1b] Error: Instance openhands-runtime-7efbf261-c0d5-4ed3-82ae-144e1c9bea1b FAILED to start container!

21:52:42 - openhands:ERROR: base.py:98 - [runtime 7efbf261-c0d5-4ed3-82ae-144e1c9bea1b] 409 Client Error for http+docker://localhost/v1.49/containers/create?name=openhands-runtime-7efbf261-c0d5-4ed3-82ae-144e1c9bea1b: Conflict ("Conflict. The container name "/openhands-runtime-7efbf261-c0d5-4ed3-82ae-144e1c9bea1b" is already in use by container "e84c484233a8d3213f0a9c941e8999dacfa2b83284b44b545b4ebea320882dc0". You have to remove (or rename) that container to be able to reuse that name.")

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

21:52:47 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f97324b6de0>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #6 | You can customize retry values in the configuration.
21:52:51 - openhands:INFO: base.py:98 - [runtime 7efbf261-c0d5-4ed3-82ae-144e1c9bea1b] Container started: openhands-runtime-7efbf261-c0d5-4ed3-82ae-144e1c9bea1b
21:52:51 - openhands:INFO: base.py:98 - [runtime 7efbf261-c0d5-4ed3-82ae-144e1c9bea1b] Waiting for client to become ready at http://host.docker.internal:35712...
21:53:23 - openhands:INFO: base.py:98 - [runtime 7efbf261-c0d5-4ed3-82ae-144e1c9bea1b] Runtime is ready.
21:53:23 - openhands:WARNING: state.py:118 - Failed to restore state from session: sessions/7efbf261-c0d5-4ed3-82ae-144e1c9bea1b/agent_state.pkl
21:53:23 - openhands:INFO: agent_controller.py:135 - [Agent Controller 7efbf261-c0d5-4ed3-82ae-144e1c9bea1b] Starting step loop...
21:53:23 - openhands:INFO: agent_controller.py:135 - [Agent Controller 7efbf261-c0d5-4ed3-82ae-144e1c9bea1b] Setting agent(CodeActAgent) state from AgentState.LOADING to AgentState.INIT
21:53:23 - openhands:INFO: github.py:14 - Initializing UserVerifier
21:53:23 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
21:53:23 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
21:53:23 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     172.17.0.1:35938 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:35934 - "GET /api/list-files HTTP/1.1" 200 OK
21:53:23 - openhands:INFO: github.py:14 - Initializing UserVerifier
21:53:23 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
21:53:23 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
21:53:23 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     172.17.0.1:35938 - "GET /api/list-files HTTP/1.1" 200 OK
21:53:31 - openhands:INFO: agent_controller.py:135 - [Agent Controller 7efbf261-c0d5-4ed3-82ae-144e1c9bea1b] Setting agent(CodeActAgent) state from AgentState.INIT to AgentState.RUNNING
21:53:31 - openhands:INFO: github.py:14 - Initializing UserVerifier
21:53:31 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
21:53:31 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
21:53:31 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users


==============
[Agent Controller 7efbf261-c0d5-4ed3-82ae-144e1c9bea1b] LEVEL 0 LOCAL STEP 0 GLOBAL STEP 0


Provider List: https://docs.litellm.ai/docs/providers


Provider List: https://docs.litellm.ai/docs/providers


Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

21:53:31 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f972174f980>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #1 | You can customize retry values in the configuration.
INFO:     172.17.0.1:35954 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:35952 - "GET /api/list-files HTTP/1.1" 200 OK

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

21:53:46 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9721785dc0>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #2 | You can customize retry values in the configuration.

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

21:53:55 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f97217800b0>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #7 | You can customize retry values in the configuration.

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

21:54:04 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9721785340>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #3 | You can customize retry values in the configuration.

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

21:54:19 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9721786e40>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #4 | You can customize retry values in the configuration.

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

21:54:35 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9721785520>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #5 | You can customize retry values in the configuration.
21:54:57 - openhands:ERROR: agent_session.py:191 - Runtime initialization failed: HTTPConnectionPool(host='host.docker.internal', port=33243): Max retries exceeded with url: /alive (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9721780e00>: Failed to establish a new connection: [Errno 101] Network is unreachable'))
Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 199, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
OSError: [Errno 101] Network is unreachable

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 789, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 495, in _make_request
    conn.request(
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 441, in request
    self.endheaders()
  File "/usr/local/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/local/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/local/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 279, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 214, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f9721780e00>: Failed to establish a new connection: [Errno 101] Network is unreachable

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 843, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='host.docker.internal', port=33243): Max retries exceeded with url: /alive (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9721780e00>: Failed to establish a new connection: [Errno 101] Network is unreachable'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/openhands/server/session/agent_session.py", line 189, in _create_runtime
    await self.runtime.connect()
  File "/app/openhands/runtime/impl/eventstream/eventstream_runtime.py", line 217, in connect
    self._wait_until_alive()
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 336, in wrapped_f
    return copy(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 475, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 478, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/runtime/impl/eventstream/eventstream_runtime.py", line 395, in _wait_until_alive
    response = send_request_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/runtime/utils/request.py", line 96, in send_request_with_retry
    return _send_request_with_retry()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 336, in wrapped_f
    return copy(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 475, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 478, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/runtime/utils/request.py", line 92, in _send_request_with_retry
    response = session.request(method, url, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/adapters.py", line 700, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='host.docker.internal', port=33243): Max retries exceeded with url: /alive (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9721780e00>: Failed to establish a new connection: [Errno 101] Network is unreachable'))
ERROR:asyncio:Future exception was never retrieved
future: <Future finished exception=ConnectionError(MaxRetryError("HTTPConnectionPool(host='host.docker.internal', port=33243): Max retries exceeded with url: /alive (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9721780e00>: Failed to establish a new connection: [Errno 101] Network is unreachable'))"))>
Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 199, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
OSError: [Errno 101] Network is unreachable

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 789, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 495, in _make_request
    conn.request(
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 441, in request
    self.endheaders()
  File "/usr/local/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/local/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/local/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 279, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 214, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f9721780e00>: Failed to establish a new connection: [Errno 101] Network is unreachable

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 843, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='host.docker.internal', port=33243): Max retries exceeded with url: /alive (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9721780e00>: Failed to establish a new connection: [Errno 101] Network is unreachable'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.12/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/server/session/agent_session.py", line 88, in _start_thread
    asyncio.run(self._start(*args), debug=True)
  File "/usr/local/lib/python3.12/asyncio/runners.py", line 194, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/asyncio/base_events.py", line 687, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/app/openhands/server/session/agent_session.py", line 105, in _start
    await self._create_runtime(
  File "/app/openhands/server/session/agent_session.py", line 189, in _create_runtime
    await self.runtime.connect()
  File "/app/openhands/runtime/impl/eventstream/eventstream_runtime.py", line 217, in connect
    self._wait_until_alive()
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 336, in wrapped_f
    return copy(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 475, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 478, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/runtime/impl/eventstream/eventstream_runtime.py", line 395, in _wait_until_alive
    response = send_request_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/runtime/utils/request.py", line 96, in send_request_with_retry
    return _send_request_with_retry()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 336, in wrapped_f
    return copy(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 475, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 478, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/runtime/utils/request.py", line 92, in _send_request_with_retry
    response = session.request(method, url, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/adapters.py", line 700, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='host.docker.internal', port=33243): Max retries exceeded with url: /alive (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9721780e00>: Failed to establish a new connection: [Errno 101] Network is unreachable'))

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

21:55:10 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f97217872c0>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #6 | You can customize retry values in the configuration.

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 199, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 789, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 495, in _make_request
    conn.request(
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 441, in request
    self.endheaders()
  File "/usr/local/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/local/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/local/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 279, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 214, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f9721781760>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 843, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9721781760>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/litellm/main.py", line 2709, in completion
    generator = ollama.get_ollama_response(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/llms/ollama.py", line 316, in get_ollama_response
    response = requests.post(
               ^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/adapters.py", line 700, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9721781760>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/openhands/controller/agent_controller.py", line 168, in start_step_loop
    await self._step()
  File "/app/openhands/controller/agent_controller.py", line 464, in _step
    action = self.agent.step(self.state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/agenthub/codeact_agent/codeact_agent.py", line 359, in step
    response = self.llm.completion(**params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 336, in wrapped_f
    return copy(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 475, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 478, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/llm/llm.py", line 196, in wrapper
    resp: ModelResponse = completion_unwrapped(*args, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/utils.py", line 1013, in wrapper
    raise e
  File "/app/.venv/lib/python3.12/site-packages/litellm/utils.py", line 903, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/main.py", line 2999, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2116, in exception_type
    raise e
  File "/app/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 1785, in exception_type
    raise ServiceUnavailableError(
litellm.exceptions.ServiceUnavailableError: litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9721781760>: Failed to establish a new connection: [Errno 111] Connection refused'))
21:56:02 - openhands:ERROR: agent_controller.py:135 - [Agent Controller c08adb0a-d9b2-4563-866f-0140cc503d3e] Error while running the agent: litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9721781760>: Failed to establish a new connection: [Errno 111] Connection refused'))
21:56:02 - openhands:ERROR: agent_controller.py:135 - [Agent Controller c08adb0a-d9b2-4563-866f-0140cc503d3e] Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 199, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 789, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 495, in _make_request
    conn.request(
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 441, in request
    self.endheaders()
  File "/usr/local/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/local/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/local/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 279, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 214, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f9721781760>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 843, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9721781760>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/litellm/main.py", line 2709, in completion
    generator = ollama.get_ollama_response(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/llms/ollama.py", line 316, in get_ollama_response
    response = requests.post(
               ^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/adapters.py", line 700, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9721781760>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/openhands/controller/agent_controller.py", line 168, in start_step_loop
    await self._step()
  File "/app/openhands/controller/agent_controller.py", line 464, in _step
    action = self.agent.step(self.state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/agenthub/codeact_agent/codeact_agent.py", line 359, in step
    response = self.llm.completion(**params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 336, in wrapped_f
    return copy(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 475, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 478, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/llm/llm.py", line 196, in wrapper
    resp: ModelResponse = completion_unwrapped(*args, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/utils.py", line 1013, in wrapper
    raise e
  File "/app/.venv/lib/python3.12/site-packages/litellm/utils.py", line 903, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/main.py", line 2999, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2116, in exception_type
    raise e
  File "/app/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 1785, in exception_type
    raise ServiceUnavailableError(
litellm.exceptions.ServiceUnavailableError: litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9721781760>: Failed to establish a new connection: [Errno 111] Connection refused'))

21:56:02 - openhands:INFO: agent_controller.py:135 - [Agent Controller c08adb0a-d9b2-4563-866f-0140cc503d3e] Setting agent(CodeActAgent) state from AgentState.RUNNING to AgentState.ERROR

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

21:56:17 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9721787320>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #7 | You can customize retry values in the configuration.
^CINFO:     Shutting down
21:57:58 - openhands:INFO: session.py:66 - WebSocket disconnected, sid: 7efbf261-c0d5-4ed3-82ae-144e1c9bea1b
INFO:     connection closed
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [10]

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 199, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 789, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 495, in _make_request
    conn.request(
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 441, in request
    self.endheaders()
  File "/usr/local/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/local/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/local/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 279, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 214, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f97324b7170>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 843, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f97324b7170>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/litellm/main.py", line 2709, in completion
    generator = ollama.get_ollama_response(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/llms/ollama.py", line 316, in get_ollama_response
    response = requests.post(
               ^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/adapters.py", line 700, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f97324b7170>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/openhands/controller/agent_controller.py", line 168, in start_step_loop
    await self._step()
  File "/app/openhands/controller/agent_controller.py", line 464, in _step
    action = self.agent.step(self.state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/agenthub/codeact_agent/codeact_agent.py", line 359, in step
    response = self.llm.completion(**params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 336, in wrapped_f
    return copy(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 475, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 478, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/llm/llm.py", line 196, in wrapper
    resp: ModelResponse = completion_unwrapped(*args, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/utils.py", line 1013, in wrapper
    raise e
  File "/app/.venv/lib/python3.12/site-packages/litellm/utils.py", line 903, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/main.py", line 2999, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2116, in exception_type
    raise e
  File "/app/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 1785, in exception_type
    raise ServiceUnavailableError(
litellm.exceptions.ServiceUnavailableError: litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f97324b7170>: Failed to establish a new connection: [Errno 111] Connection refused'))
21:58:24 - openhands:ERROR: agent_controller.py:135 - [Agent Controller 7efbf261-c0d5-4ed3-82ae-144e1c9bea1b] Error while running the agent: litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f97324b7170>: Failed to establish a new connection: [Errno 111] Connection refused'))
21:58:24 - openhands:ERROR: agent_controller.py:135 - [Agent Controller 7efbf261-c0d5-4ed3-82ae-144e1c9bea1b] Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 199, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 789, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 495, in _make_request
    conn.request(
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 441, in request
    self.endheaders()
  File "/usr/local/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/local/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/local/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 279, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 214, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f97324b7170>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 843, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f97324b7170>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/litellm/main.py", line 2709, in completion
    generator = ollama.get_ollama_response(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/llms/ollama.py", line 316, in get_ollama_response
    response = requests.post(
               ^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/adapters.py", line 700, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f97324b7170>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/openhands/controller/agent_controller.py", line 168, in start_step_loop
    await self._step()
  File "/app/openhands/controller/agent_controller.py", line 464, in _step
    action = self.agent.step(self.state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/agenthub/codeact_agent/codeact_agent.py", line 359, in step
    response = self.llm.completion(**params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 336, in wrapped_f
    return copy(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 475, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 478, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/llm/llm.py", line 196, in wrapper
    resp: ModelResponse = completion_unwrapped(*args, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/utils.py", line 1013, in wrapper
    raise e
  File "/app/.venv/lib/python3.12/site-packages/litellm/utils.py", line 903, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/main.py", line 2999, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2116, in exception_type
    raise e
  File "/app/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 1785, in exception_type
    raise ServiceUnavailableError(
litellm.exceptions.ServiceUnavailableError: litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f97324b7170>: Failed to establish a new connection: [Errno 111] Connection refused'))

21:58:24 - openhands:INFO: agent_controller.py:135 - [Agent Controller 7efbf261-c0d5-4ed3-82ae-144e1c9bea1b] Setting agent(CodeActAgent) state from AgentState.RUNNING to AgentState.ERROR
m@DESKTOP-MUB16F8:~$ docker run -it --privileged --rm \
LLM_API>   -e LLM_API_KEY="ollama" \
>   -e OLLAMA_HOST="http://172.21.151.42:11434" \
>   -e LLM_OLLAMA_BASE_URL="http://172.21.151.42:11434" \
>   -e OLLAMA_BASE_URL="http://172.21.151.42:11434" \
>   -e OLLAMA_URL="http://172.21.151.42:11434" \
>   -e SANDBOX_RUNTIME_CONTAINER_IMAGE="docker.all-hands.dev/all-hands-ai/runtime:0.12-nikolaik" \
>   -v /var/run/docker.sock:/var/run/docker.sock \
>   -p 3000:3000 \
>   --name openhands-app \
>   docker.all-hands.dev/all-hands-ai/openhands:0.12
Starting OpenHands...
Running OpenHands as root
INFO:     Started server process [10]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:3000 (Press CTRL+C to quit)
INFO:     172.17.0.1:48974 - "GET / HTTP/1.1" 200 OK
INFO:     172.17.0.1:48974 - "GET /locales/en/translation.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:48974 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:48984 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:48984 - "GET /config.json HTTP/1.1" 200 OK
21:59:35 - openhands:INFO: github.py:14 - Initializing UserVerifier
21:59:35 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
21:59:35 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
21:59:35 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     ('172.17.0.1', 48990) - "WebSocket /ws" [accepted]
21:59:35 - openhands:ERROR: auth.py:27 - Invalid token
INFO:     connection open
INFO:     172.17.0.1:48984 - "GET /api/options/models HTTP/1.1" 200 OK
INFO:     172.17.0.1:48974 - "GET /api/options/agents HTTP/1.1" 200 OK
INFO:     172.17.0.1:49004 - "GET /api/options/security-analyzers HTTP/1.1" 200 OK
INFO:     connection closed
INFO:     172.17.0.1:49004 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:48974 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:49010 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:49012 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:49012 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:49010 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:49010 - "GET /config.json HTTP/1.1" 200 OK
21:59:46 - openhands:INFO: github.py:14 - Initializing UserVerifier
21:59:46 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
21:59:46 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
21:59:46 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     ('172.17.0.1', 54666) - "WebSocket /ws" [accepted]
21:59:46 - openhands:INFO: listen.py:336 - New session: 5bd2485c-984e-4453-b636-3de44ced3087
INFO:     connection open
21:59:46 - openhands:WARNING: codeact_agent.py:101 - Function calling not supported for model ollama/llama3:latest. Disabling function calling.
INFO:     172.17.0.1:49010 - "GET /config.json HTTP/1.1" 200 OK
21:59:46 - openhands:INFO: base.py:98 - [runtime 5bd2485c-984e-4453-b636-3de44ced3087] Starting runtime with image: docker.all-hands.dev/all-hands-ai/runtime:0.12-nikolaik
21:59:47 - openhands:INFO: base.py:98 - [runtime 5bd2485c-984e-4453-b636-3de44ced3087] Container started: openhands-runtime-5bd2485c-984e-4453-b636-3de44ced3087
21:59:47 - openhands:INFO: base.py:98 - [runtime 5bd2485c-984e-4453-b636-3de44ced3087] Waiting for client to become ready at http://host.docker.internal:37702...
22:00:19 - openhands:INFO: base.py:98 - [runtime 5bd2485c-984e-4453-b636-3de44ced3087] Runtime is ready.
22:00:19 - openhands:WARNING: state.py:118 - Failed to restore state from session: sessions/5bd2485c-984e-4453-b636-3de44ced3087/agent_state.pkl
22:00:19 - openhands:INFO: agent_controller.py:135 - [Agent Controller 5bd2485c-984e-4453-b636-3de44ced3087] Starting step loop...
22:00:19 - openhands:INFO: agent_controller.py:135 - [Agent Controller 5bd2485c-984e-4453-b636-3de44ced3087] Setting agent(CodeActAgent) state from AgentState.LOADING to AgentState.INIT
22:00:19 - openhands:INFO: agent_controller.py:135 - [Agent Controller 5bd2485c-984e-4453-b636-3de44ced3087] Setting agent(CodeActAgent) state from AgentState.INIT to AgentState.RUNNING
22:00:19 - openhands:INFO: github.py:14 - Initializing UserVerifier
22:00:19 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
22:00:19 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
22:00:19 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     172.17.0.1:37572 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:37584 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:37562 - "GET /api/list-files HTTP/1.1" 200 OK
22:00:19 - openhands:INFO: github.py:14 - Initializing UserVerifier
22:00:19 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
22:00:19 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
22:00:19 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     172.17.0.1:37584 - "GET /api/list-files HTTP/1.1" 200 OK
22:00:20 - openhands:INFO: github.py:14 - Initializing UserVerifier
22:00:20 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
22:00:20 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
22:00:20 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     172.17.0.1:37562 - "GET /api/list-files HTTP/1.1" 200 OK


==============
[Agent Controller 5bd2485c-984e-4453-b636-3de44ced3087] LEVEL 0 LOCAL STEP 0 GLOBAL STEP 0


Provider List: https://docs.litellm.ai/docs/providers


Provider List: https://docs.litellm.ai/docs/providers


Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

22:00:20 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fa1b7683ec0>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #1 | You can customize retry values in the configuration.

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

22:00:35 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fa1b7675ca0>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #2 | You can customize retry values in the configuration.

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

22:00:52 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fa1b750bf50>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #3 | You can customize retry values in the configuration.

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

22:01:07 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fa1b75216d0>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #4 | You can customize retry values in the configuration.

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

22:01:25 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fa1b750b7a0>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #5 | You can customize retry values in the configuration.

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

22:01:59 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fa1b75208c0>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #6 | You can customize retry values in the configuration.
22:02:37 - openhands:INFO: agent_controller.py:135 - [Agent Controller 5bd2485c-984e-4453-b636-3de44ced3087] Setting agent(CodeActAgent) state from AgentState.RUNNING to paused
22:02:37 - openhands:INFO: github.py:14 - Initializing UserVerifier
22:02:37 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
22:02:37 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
22:02:37 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     172.17.0.1:47052 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:47038 - "GET /api/list-files HTTP/1.1" 200 OK
22:02:38 - openhands:INFO: agent_controller.py:135 - [Agent Controller 5bd2485c-984e-4453-b636-3de44ced3087] Setting agent(CodeActAgent) state from paused to running
22:02:38 - openhands:INFO: github.py:14 - Initializing UserVerifier
22:02:38 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
22:02:38 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
22:02:38 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     172.17.0.1:47052 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:47038 - "GET /api/list-files HTTP/1.1" 200 OK

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

22:03:06 - openhands:ERROR: retry_mixin.py:47 - litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fa1b7532ea0>: Failed to establish a new connection: [Errno 111] Connection refused')). Attempt #7 | You can customize retry values in the configuration.

Give Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new
LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.


Provider List: https://docs.litellm.ai/docs/providers

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 199, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 789, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 495, in _make_request
    conn.request(
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 441, in request
    self.endheaders()
  File "/usr/local/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/local/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/local/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 279, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 214, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7fa1b76411c0>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 843, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fa1b76411c0>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/litellm/main.py", line 2709, in completion
    generator = ollama.get_ollama_response(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/llms/ollama.py", line 316, in get_ollama_response
    response = requests.post(
               ^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/adapters.py", line 700, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fa1b76411c0>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/openhands/controller/agent_controller.py", line 168, in start_step_loop
    await self._step()
  File "/app/openhands/controller/agent_controller.py", line 464, in _step
    action = self.agent.step(self.state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/agenthub/codeact_agent/codeact_agent.py", line 359, in step
    response = self.llm.completion(**params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 336, in wrapped_f
    return copy(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 475, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 478, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/llm/llm.py", line 196, in wrapper
    resp: ModelResponse = completion_unwrapped(*args, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/utils.py", line 1013, in wrapper
    raise e
  File "/app/.venv/lib/python3.12/site-packages/litellm/utils.py", line 903, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/main.py", line 2999, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2116, in exception_type
    raise e
  File "/app/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 1785, in exception_type
    raise ServiceUnavailableError(
litellm.exceptions.ServiceUnavailableError: litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fa1b76411c0>: Failed to establish a new connection: [Errno 111] Connection refused'))
22:05:14 - openhands:ERROR: agent_controller.py:135 - [Agent Controller 5bd2485c-984e-4453-b636-3de44ced3087] Error while running the agent: litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fa1b76411c0>: Failed to establish a new connection: [Errno 111] Connection refused'))
22:05:14 - openhands:ERROR: agent_controller.py:135 - [Agent Controller 5bd2485c-984e-4453-b636-3de44ced3087] Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 199, in _new_conn
    sock = connection.create_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 789, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 495, in _make_request
    conn.request(
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 441, in request
    self.endheaders()
  File "/usr/local/lib/python3.12/http/client.py", line 1331, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/local/lib/python3.12/http/client.py", line 1091, in _send_output
    self.send(msg)
  File "/usr/local/lib/python3.12/http/client.py", line 1035, in send
    self.connect()
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 279, in connect
    self.sock = self._new_conn()
                ^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connection.py", line 214, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7fa1b76411c0>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py", line 843, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fa1b76411c0>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/.venv/lib/python3.12/site-packages/litellm/main.py", line 2709, in completion
    generator = ollama.get_ollama_response(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/llms/ollama.py", line 316, in get_ollama_response
    response = requests.post(
               ^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/api.py", line 115, in post
    return request("post", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/requests/adapters.py", line 700, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fa1b76411c0>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/openhands/controller/agent_controller.py", line 168, in start_step_loop
    await self._step()
  File "/app/openhands/controller/agent_controller.py", line 464, in _step
    action = self.agent.step(self.state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/agenthub/codeact_agent/codeact_agent.py", line 359, in step
    response = self.llm.completion(**params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 336, in wrapped_f
    return copy(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 475, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/app/.venv/lib/python3.12/site-packages/tenacity/__init__.py", line 478, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/app/openhands/llm/llm.py", line 196, in wrapper
    resp: ModelResponse = completion_unwrapped(*args, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/utils.py", line 1013, in wrapper
    raise e
  File "/app/.venv/lib/python3.12/site-packages/litellm/utils.py", line 903, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/main.py", line 2999, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/app/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2116, in exception_type
    raise e
  File "/app/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 1785, in exception_type
    raise ServiceUnavailableError(
litellm.exceptions.ServiceUnavailableError: litellm.ServiceUnavailableError: OllamaException: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fa1b76411c0>: Failed to establish a new connection: [Errno 111] Connection refused'))

22:05:14 - openhands:INFO: agent_controller.py:135 - [Agent Controller 5bd2485c-984e-4453-b636-3de44ced3087] Setting agent(CodeActAgent) state from running to AgentState.ERROR
^CINFO:     Shutting down
22:06:55 - openhands:INFO: session.py:66 - WebSocket disconnected, sid: 5bd2485c-984e-4453-b636-3de44ced3087
INFO:     connection closed
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [10]
^CException ignored in atexit callback: <bound method EventStreamRuntime.close of <openhands.runtime.impl.eventstream.eventstream_runtime.EventStreamRuntime object at 0x7fa1b75237d0>>
Traceback (most recent call last):
  File "/app/openhands/runtime/impl/eventstream/eventstream_runtime.py", line 417, in close
    self.log_buffer.close()
  File "/app/openhands/runtime/impl/eventstream/eventstream_runtime.py", line 100, in close
    self.log_stream_thread.join(timeout)
  File "/usr/local/lib/python3.12/threading.py", line 1151, in join
    self._wait_for_tstate_lock(timeout=max(timeout, 0))
  File "/usr/local/lib/python3.12/threading.py", line 1167, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt:
^CException ignored in atexit callback: <bound method EventStreamRuntime.close of <openhands.runtime.impl.eventstream.eventstream_runtime.EventStreamRuntime object at 0x7fa1b7bac2f0>>
Traceback (most recent call last):
  File "/app/openhands/runtime/impl/eventstream/eventstream_runtime.py", line 417, in close
    self.log_buffer.close()
  File "/app/openhands/runtime/impl/eventstream/eventstream_runtime.py", line 100, in close
    self.log_stream_thread.join(timeout)
  File "/usr/local/lib/python3.12/threading.py", line 1151, in join
    self._wait_for_tstate_lock(timeout=max(timeout, 0))
  File "/usr/local/lib/python3.12/threading.py", line 1167, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt:
m@DESKTOP-MUB16F8:~$ docker run -it --privileged --rm \
LLM_API>   -e LLM_API_KEY="ollama" \
>   -e OLLAMA_HOST="http://172.21.151.42:11434" \
>   -e LLM_OLLAMA_BASE_URL="http://172.21.151.42:11434" \
>   -e OLLAMA_BASE_URL="http://172.21.151.42:11434" \
>   -e OLLAMA_URL="http://172.21.151.42:11434" \
>   -e SANDBOX_RUNTIME_CONTAINER_IMAGE="docker.all-hands.dev/all-hands-ai/runtime:0.12-nikolaik" \
>   -v /var/run/docker.sock:/var/run/docker.sock \
>   -p 3000:3000 \
>   --name openhands-app \
>   docker.all-hands.dev/all-hands-ai/openhands:0.12
Starting OpenHands...
Running OpenHands as root
INFO:     Started server process [10]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:3000 (Press CTRL+C to quit)
INFO:     172.17.0.1:34962 - "GET / HTTP/1.1" 200 OK
INFO:     172.17.0.1:34962 - "GET /locales/en/translation.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:34962 - "GET /favicon.ico HTTP/1.1" 200 OK
INFO:     172.17.0.1:34962 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:34974 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:34974 - "GET /config.json HTTP/1.1" 200 OK
22:08:17 - openhands:INFO: github.py:14 - Initializing UserVerifier
22:08:17 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
22:08:17 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
22:08:17 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     ('172.17.0.1', 34980) - "WebSocket /ws" [accepted]
22:08:17 - openhands:ERROR: auth.py:27 - Invalid token
INFO:     connection open
INFO:     172.17.0.1:34974 - "GET /api/options/models HTTP/1.1" 200 OK
INFO:     172.17.0.1:34962 - "GET /api/options/agents HTTP/1.1" 200 OK
INFO:     172.17.0.1:34982 - "GET /api/options/security-analyzers HTTP/1.1" 200 OK
INFO:     connection closed
INFO:     172.17.0.1:34982 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:34962 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:34962 - "GET / HTTP/1.1" 200 OK
INFO:     172.17.0.1:34962 - "GET /locales/en/translation.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:34962 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:34982 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:34982 - "GET /api/options/models HTTP/1.1" 200 OK
INFO:     172.17.0.1:34962 - "GET /api/options/agents HTTP/1.1" 200 OK
INFO:     172.17.0.1:34974 - "GET /api/options/security-analyzers HTTP/1.1" 200 OK
INFO:     172.17.0.1:51484 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:51484 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:51484 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:51492 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:34872 - "GET /config.json HTTP/1.1" 200 OK
22:09:02 - openhands:INFO: github.py:14 - Initializing UserVerifier
22:09:02 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
22:09:02 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
22:09:02 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     ('172.17.0.1', 34874) - "WebSocket /ws" [accepted]
22:09:02 - openhands:INFO: listen.py:336 - New session: 04d83367-dcb8-40bd-a5fb-d29a404ce676
INFO:     connection open
22:09:02 - openhands:WARNING: codeact_agent.py:101 - Function calling not supported for model ollama/llama3:latest. Disabling function calling.
INFO:     172.17.0.1:34872 - "GET /config.json HTTP/1.1" 200 OK
22:09:02 - openhands:INFO: base.py:98 - [runtime 04d83367-dcb8-40bd-a5fb-d29a404ce676] Starting runtime with image: docker.all-hands.dev/all-hands-ai/runtime:0.12-nikolaik
22:09:02 - openhands:INFO: base.py:98 - [runtime 04d83367-dcb8-40bd-a5fb-d29a404ce676] Container started: openhands-runtime-04d83367-dcb8-40bd-a5fb-d29a404ce676
22:09:02 - openhands:INFO: base.py:98 - [runtime 04d83367-dcb8-40bd-a5fb-d29a404ce676] Waiting for client to become ready at http://host.docker.internal:31789...
22:09:33 - openhands:INFO: base.py:98 - [runtime 04d83367-dcb8-40bd-a5fb-d29a404ce676] Runtime is ready.
22:09:33 - openhands:WARNING: state.py:118 - Failed to restore state from session: sessions/04d83367-dcb8-40bd-a5fb-d29a404ce676/agent_state.pkl
22:09:33 - openhands:INFO: agent_controller.py:135 - [Agent Controller 04d83367-dcb8-40bd-a5fb-d29a404ce676] Starting step loop...
22:09:33 - openhands:INFO: agent_controller.py:135 - [Agent Controller 04d83367-dcb8-40bd-a5fb-d29a404ce676] Setting agent(CodeActAgent) state from AgentState.LOADING to AgentState.INIT
22:09:33 - openhands:INFO: agent_controller.py:135 - [Agent Controller 04d83367-dcb8-40bd-a5fb-d29a404ce676] Setting agent(CodeActAgent) state from AgentState.INIT to AgentState.RUNNING
22:09:33 - openhands:INFO: github.py:14 - Initializing UserVerifier
22:09:33 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
22:09:33 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
22:09:33 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     172.17.0.1:40502 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:40510 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:40500 - "GET /api/list-files HTTP/1.1" 200 OK
22:09:33 - openhands:INFO: github.py:14 - Initializing UserVerifier
22:09:33 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
22:09:33 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
22:09:33 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     172.17.0.1:40510 - "GET /api/list-files HTTP/1.1" 200 OK
22:09:34 - openhands:INFO: github.py:14 - Initializing UserVerifier
22:09:34 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
22:09:34 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
22:09:34 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     172.17.0.1:40500 - "GET /api/list-files HTTP/1.1" 200 OK


==============
[Agent Controller 04d83367-dcb8-40bd-a5fb-d29a404ce676] LEVEL 0 LOCAL STEP 0 GLOBAL STEP 0


Provider List: https://docs.litellm.ai/docs/providers


Provider List: https://docs.litellm.ai/docs/providers

22:13:52 - openhands:INFO: agent_controller.py:135 - [Agent Controller 04d83367-dcb8-40bd-a5fb-d29a404ce676] Setting agent(CodeActAgent) state from AgentState.RUNNING to AgentState.AWAITING_USER_INPUT
22:13:52 - openhands:INFO: github.py:14 - Initializing UserVerifier
22:13:52 - openhands:INFO: github.py:27 - GITHUB_USER_LIST_FILE not configured
22:13:52 - openhands:INFO: github.py:48 - GITHUB_USERS_SHEET_ID not configured
22:13:52 - openhands:INFO: github.py:85 - No user verification sources configured - allowing all users
INFO:     172.17.0.1:56098 - "GET /config.json HTTP/1.1" 200 OK
INFO:     172.17.0.1:56090 - "GET /api/list-files HTTP/1.1" 200 OK









###############

m@DESKTOP-MUB16F8:~$ OLLAMA_HOST=0.0.0.0 ollama serve
Error: listen tcp 0.0.0.0:11434: bind: address already in use
m@DESKTOP-MUB16F8:~$ sudo systemctl stop ollama
[sudo] password for m:
Sorry, try again.
[sudo] password for m:
m@DESKTOP-MUB16F8:~$ ps aux | grep ollama
m         4835  0.0  0.0   4092  1952 pts/4    S+   13:53   0:00 grep --color=auto ollama
m@DESKTOP-MUB16F8:~$ sudo kill -9 4835
kill: (4835): No such process
m@DESKTOP-MUB16F8:~$ sudo kill -9 4769
kill: (4769): No such process
m@DESKTOP-MUB16F8:~$ ps aux | grep ollama
do lsof -i :11434m         4843  0.0  0.0   4092  1840 pts/4    S+   13:53   0:00 grep --color=auto ollama
m@DESKTOP-MUB16F8:~$ sudo lsof -i :11434
m@DESKTOP-MUB16F8:~$ OLLAMA_HOST=0.0.0.0 ollama serve
time=2025-05-26T13:54:14.061-07:00 level=INFO source=routes.go:1205 msg="server config" env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/m/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-05-26T13:54:14.066-07:00 level=INFO source=images.go:463 msg="total blobs: 0"
time=2025-05-26T13:54:14.066-07:00 level=INFO source=images.go:470 msg="total unused blobs removed: 0"
time=2025-05-26T13:54:14.067-07:00 level=INFO source=routes.go:1258 msg="Listening on [::]:11434 (version 0.7.1)"
time=2025-05-26T13:54:14.067-07:00 level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-05-26T13:54:19.453-07:00 level=INFO source=gpu.go:377 msg="no compatible GPUs were discovered"
time=2025-05-26T13:54:19.453-07:00 level=INFO source=types.go:130 msg="inference compute" id=0 library=cpu variant="" compute="" driver=0.0 name="" total="31.3 GiB" available="29.7 GiB"
[GIN] 2025/05/26 - 13:54:31 | 200 |    1.836124ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/05/26 - 13:55:04 | 200 |     186.208µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/05/26 - 13:55:05 | 200 |     117.219µs |   172.21.151.42 | GET      "/api/tags"
[GIN] 2025/05/26 - 13:55:19 | 200 |      101.48µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/05/26 - 13:55:22 | 200 |     133.932µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/05/26 - 13:55:31 | 200 |     125.854µs |   172.21.151.42 | GET      "/api/tags"
[GIN] 2025/05/26 - 13:55:34 | 200 |     100.679µs |   172.21.151.42 | GET      "/api/tags"
[GIN] 2025/05/26 - 13:56:43 | 200 |      33.926µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/05/26 - 13:56:43 | 200 |     119.351µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/05/26 - 13:57:15 | 200 |      75.635µs |       127.0.0.1 | HEAD     "/"
time=2025-05-26T13:57:17.037-07:00 level=INFO source=download.go:177 msg="downloading 6a0746a1ec1a in 16 291 MB part(s)"
time=2025-05-26T13:59:16.618-07:00 level=INFO source=download.go:295 msg="6a0746a1ec1a part 1 attempt 0 failed: unexpected EOF, retrying in 1s"
time=2025-05-26T14:15:19.361-07:00 level=INFO source=download.go:374 msg="6a0746a1ec1a part 14 stalled; retrying. If this persists, press ctrl-c to exit, then 'ollama pull' to find a faster connection."
time=2025-05-26T14:20:25.905-07:00 level=INFO source=download.go:177 msg="downloading 4fa551d4f938 in 1 12 KB part(s)"
time=2025-05-26T14:20:27.273-07:00 level=INFO source=download.go:177 msg="downloading 8ab4849b038c in 1 254 B part(s)"
time=2025-05-26T14:20:28.692-07:00 level=INFO source=download.go:177 msg="downloading 577073ffcc6c in 1 110 B part(s)"
time=2025-05-26T14:20:30.038-07:00 level=INFO source=download.go:177 msg="downloading 3f8eb4da87fa in 1 485 B part(s)"
[GIN] 2025/05/26 - 14:20:43 | 200 |         22m5s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/05/26 - 14:21:32 | 200 |       23.58µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/05/26 - 14:21:32 | 200 |      306.27µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/05/26 - 14:23:20 | 200 |    7.360771ms |    172.21.144.1 | GET      "/api/tags"
[GIN] 2025/05/26 - 14:23:50 | 200 |    5.275202ms |    172.21.144.1 | GET      "/api/tags"
[GIN] 2025/05/26 - 14:31:18 | 200 |    4.458427ms |    172.21.144.1 | GET      "/api/tags"
[GIN] 2025/05/26 - 14:41:17 | 200 |    4.601623ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/05/26 - 14:41:17 | 200 |     685.662µs |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/05/26 - 14:45:57 | 200 |    4.449886ms |    172.21.144.1 | GET      "/api/tags"
[GIN] 2025/05/26 - 14:47:40 | 200 |      388.29µs |    172.21.144.1 | GET      "/api/tags"
[GIN] 2025/05/26 - 14:52:41 | 200 |    4.798266ms |    172.21.144.1 | GET      "/api/tags"
time=2025-05-26T14:55:17.934-07:00 level=INFO source=server.go:135 msg="system memory" total="31.3 GiB" free="28.3 GiB" free_swap="8.0 GiB"
time=2025-05-26T14:55:17.936-07:00 level=INFO source=server.go:168 msg=offload library=cpu layers.requested=-1 layers.model=33 layers.offload=0 layers.split="" memory.available="[28.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.8 GiB" memory.required.partial="0 B" memory.required.kv="1.0 GiB" memory.required.allocations="[5.8 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="411.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/m/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 4.33 GiB (4.64 BPW)
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta-Llama-3-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-05-26T14:55:18.305-07:00 level=INFO source=server.go:431 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/m/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa --ctx-size 8192 --batch-size 512 --threads 14 --no-mmap --parallel 2 --port 37965"
time=2025-05-26T14:55:18.308-07:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-05-26T14:55:18.308-07:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-05-26T14:55:18.308-07:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server not responding"
time=2025-05-26T14:55:18.356-07:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
time=2025-05-26T14:55:18.403-07:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
time=2025-05-26T14:55:18.439-07:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:37965"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/m/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 4.33 GiB (4.64 BPW)
time=2025-05-26T14:55:18.560-07:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 8192
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 8192
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta-Llama-3-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors:          CPU model buffer size =  4437.80 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     1.01 MiB
llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32
llama_kv_cache_unified:        CPU KV buffer size =  1024.00 MiB
llama_kv_cache_unified: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_context:        CPU compute buffer size =   560.01 MiB
llama_context: graph nodes  = 1094
llama_context: graph splits = 1
time=2025-05-26T14:55:39.215-07:00 level=INFO source=server.go:630 msg="llama runner started in 19.88 seconds"
[GIN] 2025/05/26 - 14:55:45 | 200 | 26.904802309s |    172.21.144.1 | POST     "/api/generate"
[GIN] 2025/05/26 - 14:59:35 | 200 |    1.835348ms |    172.21.144.1 | GET      "/api/tags"
time=2025-05-26T15:01:53.133-07:00 level=INFO source=server.go:135 msg="system memory" total="31.3 GiB" free="28.3 GiB" free_swap="8.0 GiB"
time=2025-05-26T15:01:53.134-07:00 level=INFO source=server.go:168 msg=offload library=cpu layers.requested=-1 layers.model=33 layers.offload=0 layers.split="" memory.available="[28.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.8 GiB" memory.required.partial="0 B" memory.required.kv="1.0 GiB" memory.required.allocations="[5.8 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="411.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/m/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 4.33 GiB (4.64 BPW)
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta-Llama-3-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-05-26T15:01:53.466-07:00 level=INFO source=server.go:431 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/m/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa --ctx-size 8192 --batch-size 512 --threads 14 --no-mmap --parallel 2 --port 39693"
time=2025-05-26T15:01:53.467-07:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-05-26T15:01:53.467-07:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-05-26T15:01:53.467-07:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server not responding"
time=2025-05-26T15:01:53.497-07:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
time=2025-05-26T15:01:53.503-07:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
time=2025-05-26T15:01:53.531-07:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:39693"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/m/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 4.33 GiB (4.64 BPW)
time=2025-05-26T15:01:53.720-07:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 8192
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 8192
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta-Llama-3-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors:          CPU model buffer size =  4437.80 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     1.01 MiB
llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32
llama_kv_cache_unified:        CPU KV buffer size =  1024.00 MiB
llama_kv_cache_unified: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_context:        CPU compute buffer size =   560.01 MiB
llama_context: graph nodes  = 1094
llama_context: graph splits = 1
time=2025-05-26T15:01:55.985-07:00 level=INFO source=server.go:630 msg="llama runner started in 2.52 seconds"
[GIN] 2025/05/26 - 15:01:57 | 200 |  4.874537738s |    172.21.144.1 | POST     "/api/generate"
[GIN] 2025/05/26 - 15:08:17 | 200 |    7.939113ms |    172.21.144.1 | GET      "/api/tags"
[GIN] 2025/05/26 - 15:08:18 | 200 |     964.133µs |    172.21.144.1 | GET      "/api/tags"
time=2025-05-26T15:09:35.005-07:00 level=INFO source=server.go:135 msg="system memory" total="31.3 GiB" free="27.2 GiB" free_swap="8.0 GiB"
time=2025-05-26T15:09:35.005-07:00 level=INFO source=server.go:168 msg=offload library=cpu layers.requested=-1 layers.model=33 layers.offload=0 layers.split="" memory.available="[27.2 GiB]" memory.gpu_overhead="0 B" memory.required.full="5.8 GiB" memory.required.partial="0 B" memory.required.kv="1.0 GiB" memory.required.allocations="[5.8 GiB]" memory.weights.total="4.1 GiB" memory.weights.repeating="3.7 GiB" memory.weights.nonrepeating="411.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/m/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 4.33 GiB (4.64 BPW)
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta-Llama-3-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-05-26T15:09:35.387-07:00 level=INFO source=server.go:431 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/m/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa --ctx-size 8192 --batch-size 512 --threads 14 --no-mmap --parallel 2 --port 37957"
time=2025-05-26T15:09:35.390-07:00 level=INFO source=sched.go:483 msg="loaded runners" count=1
time=2025-05-26T15:09:35.390-07:00 level=INFO source=server.go:591 msg="waiting for llama runner to start responding"
time=2025-05-26T15:09:35.391-07:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server not responding"
time=2025-05-26T15:09:35.436-07:00 level=INFO source=runner.go:815 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
time=2025-05-26T15:09:35.474-07:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
time=2025-05-26T15:09:35.500-07:00 level=INFO source=runner.go:874 msg="Server listening on 127.0.0.1:37957"
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/m/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 4.33 GiB (4.64 BPW)
time=2025-05-26T15:09:35.643-07:00 level=INFO source=server.go:625 msg="waiting for server to become available" status="llm server loading model"
load: special tokens cache size = 256
load: token to piece cache size = 0.8000 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 8192
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: n_swa_pattern    = 1
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 8192
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta-Llama-3-8B-Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128009 '<|eot_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = false)
load_tensors:          CPU model buffer size =  4437.80 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 1024
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     1.01 MiB
llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32
llama_kv_cache_unified:        CPU KV buffer size =  1024.00 MiB
llama_kv_cache_unified: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_context:        CPU compute buffer size =   560.01 MiB
llama_context: graph nodes  = 1094
llama_context: graph splits = 1
time=2025-05-26T15:09:57.318-07:00 level=INFO source=server.go:630 msg="llama runner started in 20.13 seconds"
time=2025-05-26T15:09:57.335-07:00 level=WARN source=runner.go:128 msg="truncating input prompt" limit=4096 prompt=4311 keep=25 new=4096
[GIN] 2025/05/26 - 15:13:52 | 200 |          4m2s |    172.21.144.1 | POST     "/api/generate"




##################
